{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7246920b-4ac5-45ff-91fd-5522c95aed6b",
   "metadata": {},
   "source": [
    "# Fauna\n",
    "\n",
    "This is a deconstruction of parts of fauna. \n",
    "\n",
    "* https://github.com/nextstrain/fauna\n",
    "\n",
    "Scripts:\n",
    "\n",
    "**zika_upload.py**\n",
    "\n",
    "```\n",
    "python3 vdb/zika_upload.py \\\n",
    "  -db vdb \\\n",
    "  -v zika \\\n",
    "  --source genbank \\\n",
    "  --locus genome \\\n",
    "  --fname GenomicFastaResults.fasta\n",
    "```\n",
    "\n",
    "**zika_update.py**\n",
    "\n",
    "```\n",
    "python3 vdb/zika_update.py \\\n",
    "  -db vdb \\\n",
    "  -v zika \\\n",
    "  --update_citations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5a191-20a9-4446-adbd-2ccb76e6335e",
   "metadata": {},
   "source": [
    "*check dependencies listed in requirements.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f3cb59-e085-4d59-af0a-4c66aedd8b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages available\n"
     ]
    }
   ],
   "source": [
    "# ==== Packages\n",
    "import os, re, time, csv, sys\n",
    "from Bio import SeqIO\n",
    "from typing import NamedTuple\n",
    "from datetime import datetime\n",
    "print(\"Packages available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf81d2fb-ff3d-4509-9a69-a59566f60b24",
   "metadata": {},
   "source": [
    "## 1. Load an example dataset\n",
    "\n",
    "Practice on 10 zika sequences. Pull from:\n",
    "\n",
    "* https://www.viprbrc.org/brc/vipr_genome_search.spg?method=ShowCleanSearch&decorator=flavi_zika"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25f90c8-e349-4c03-9db2-580062d4c1d1",
   "metadata": {},
   "source": [
    "## 2. Process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8895643-63f6-4de7-9c63-403a1478a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Input variables\n",
    "zika_fasta = \"../example_data/small.fasta\"\n",
    "\n",
    "# From fauna\n",
    "strain_fix_fname =  \"zika_strain_name_fix.tsv\"\n",
    "location_fix_fname = \"zika_location_fix.tsv\"\n",
    "date_fix_fname = \"zika_date_fix.tsv\"\n",
    "\n",
    "#virus_fasta_fields = {1:'strain', 3:'collection_date', 4: 'host', 5:'country'}\n",
    "#sequence_fasta_fields = {0:'accession', 1:'strain'}\n",
    "# Seems duplicative, replace with:\n",
    "header_fasta_fields = {0:'accession', 1:'strain', 3:'collection_date', 4: 'host', 5:'country'}\n",
    "# If we're ignoring anything past 5, then don't pull from vipr\n",
    "\n",
    "virus=\"zika\" # upload\n",
    "# ex. 2002-XX-XX or 2002-09-XX\n",
    "# ex. 2009-06 (Day unknown)\n",
    "# ex. 2009 (Month and day unknown)\n",
    "# https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\n",
    "expected_date_formats = {'%Y_%m_%d','%Y (Month and day unknown)', '%Y-%m (Day unknown)', \n",
    "                         '%Y %b %d','%Y-XX-XX', '%Y-%m-%d', '%Y-%m-%dT%H:%M:%SZ'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a866a61b-6709-4c67-9099-d21c71bcd1e4",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e76521e6-ab23-4d63-8e0e-b73b4e4163ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vdl/uploads.py\n",
    "def define_fixes_dict(fname:str) -> dict[str,str]:\n",
    "    '''\n",
    "    Open strain/location/date fixing tsv files and define corresponding dictionaries\n",
    "    '''\n",
    "    reader = csv.DictReader(filter(lambda row: row[0]!='#', open(fname)), delimiter='\\t')\n",
    "    fixes_dict = {}\n",
    "    for line in reader:\n",
    "        fixes_dict[line['label'].encode().decode('unicode-escape')] = line['fix']\n",
    "    return fixes_dict\n",
    "\n",
    "def fixes_str(original_str:str, fixes_dict:dict[str,str]={}):\n",
    "    '''\n",
    "    return the new strain name that will replace the original string\n",
    "    cannot be applied to location and date since key is based on strain name\n",
    "    '''\n",
    "    # labmda x: fixes[original_str] if original_str in fixes dict else original_str\n",
    "    if original_str in fixes_dict:\n",
    "        return fixes[original_str] \n",
    "    else:\n",
    "        return original_str\n",
    "\n",
    "# vdl/zika_uploads.py\n",
    "def fixes_strain_name(name, fixes_dict={}, fixes_tsv:str=\"\") -> (str,str): # Since we can't decide if we want strain or name\n",
    "    fixes_dict = {}\n",
    "    if(len(fixes_dict)<1 and len(fixes_tsv)>0):\n",
    "        fixes_dict = define_fixes_dict(fixes_tsv)\n",
    "    \n",
    "    # https://stackoverflow.com/questions/2484156/is-str-replace-replace-ad-nauseam-a-standard-idiom-in-python\n",
    "    original_name = name\n",
    "    name = fixes_str(original_name, fixes_dict) \n",
    "    name = name.replace('Zika_virus', '').replace('Zikavirus', '').replace('Zika virus', '').replace('Zika', '').replace('ZIKV', '')\n",
    "    name = name.replace('Human', '').replace('human', '').replace('H.sapiens_wt', '').replace('H.sapiens_tc', '').replace('Hsapiens_tc', '').replace('H.sapiens-tc', '').replace('Homo_sapiens', '').replace('Homo sapiens', '').replace('Hsapiens', '').replace('H.sapiens', '')\n",
    "    name = name.replace('/Hu/', '')\n",
    "    name = name.replace('_Asian', '').replace('_Asia', '').replace('_asian', '').replace('_asia', '')\n",
    "    name = name.replace('_URI', '').replace('_SER', '').replace('_PLA', '').replace('_MOS', '').replace('_SAL', '')\n",
    "    name = name.replace('Aaegypti_wt', 'Aedes_aegypti').replace('Aedessp', 'Aedes_sp')\n",
    "    name = name.replace(' ', '').replace('\\'', '').replace('(', '').replace(')', '').replace('//', '/').replace('__', '_').replace('.', '').replace(',', '')\n",
    "    name = re.sub('^[\\/\\_\\-]', '', name)\n",
    "    try: # ID must start with letter\n",
    "        name = 'V' + str(int(name))\n",
    "    except:\n",
    "        pass\n",
    "    name = fixes_str(name, fixes_dict)\n",
    "    return name\n",
    "\n",
    "# # vdl/parse.py Load data\n",
    "# def parse_fasta_file(fasta, virus_fasta_fields, sequence_fasta_fields, **kwargs):\n",
    "#     '''\n",
    "#     Parse FASTA file with default header formatting\n",
    "#     :return: list of documents(dictionaries of attributes) to upload\n",
    "#     '''\n",
    "#     header_fixes = False\n",
    "#     if (kwargs[\"fasta_header_fix\"]):\n",
    "#         header_fixes = {}\n",
    "#         try:\n",
    "#             with open(kwargs[\"fasta_header_fix\"], 'rU') as fh:\n",
    "#                 for line in fh:\n",
    "#                     if not line.startswith('#'):\n",
    "#                         k, v = line.strip().split(\"\\t\")\n",
    "#                         header_fixes[k] = v                \n",
    "#         except IOError:\n",
    "#             raise Exception(kwargs[\"fasta_header_fix\"], \"not found\")\n",
    "#     viruses = []\n",
    "#     sequences = []\n",
    "#     try:\n",
    "#         handle = open(fasta, 'r')\n",
    "#     except IOError:\n",
    "#         raise Exception(fasta, \"not found\")\n",
    "#     else:\n",
    "#         for record in SeqIO.parse(handle, \"fasta\"):\n",
    "#             if header_fixes:\n",
    "#                 try:\n",
    "#                     record.description = header_fixes[record.description]\n",
    "#                 except KeyError:\n",
    "#                     raise Exception(record.description, \"not in header fix file. Fatal.\")\n",
    "#             content = list(map(lambda x: x.strip(), record.description.replace(\">\", \"\").split('|')))\n",
    "#             v = {key: content[ii] if ii < len(content) else \"\" for ii, key in virus_fasta_fields.items()}\n",
    "#             s = {key: content[ii] if ii < len(content) else \"\" for ii, key in sequence_fasta_fields.items()}\n",
    "#             s['sequence'] = str(record.seq).lower()\n",
    "#             #v = self.add_virus_fields(v, **kwargs)\n",
    "#             #s = self.add_sequence_fields(s, **kwargs)\n",
    "#             sequences.append(s)\n",
    "#             viruses.append(v)\n",
    "#         handle.close()\n",
    "#     return (viruses, sequences)\n",
    "\n",
    "# === Only fix casing on the Host?\n",
    "def fix_casing(self, document): # JC\n",
    "    for field in ['host']:       # Looping over one entry...hmmmmmmm\n",
    "        if field in document and document[field] is not None:\n",
    "            document[field] = self.camelcase_to_snakecase(document[field])\n",
    "\n",
    "# ===== Main Method\n",
    "fix_name_dict = define_fixes_dict(strain_fix_fname)  # tsv file in Input\n",
    "fix_location_dict = define_fixes_dict(location_fix_fname)\n",
    "fix_date_dict = define_fixes_dict(date_fix_fname)\n",
    "\n",
    "type(fix_name_dict)\n",
    "type(fix_location_dict)\n",
    "type(fix_date_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b9e5b3-e947-480d-a565-0a506b319af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002-XX-XX\n",
      "2002-09-05\n",
      "2002-09-XX\n",
      "2002-02-04\n",
      "2002-02-04\n",
      "2002-02-04\n",
      "2002-02-04\n",
      "2002-02-04\n"
     ]
    }
   ],
   "source": [
    "def format_date(date_str):\n",
    "    '''\n",
    "    Format viruses date attribute: collection date in YYYY-MM-DD format, for example, 2016-02-28\n",
    "    Input date could be YYYY_MM_DD, reformat to YYYY-MM-DD\n",
    "    '''\n",
    "    # # ex. 2002_04_25 to 2002-04-25\n",
    "    # date_fields = []\n",
    "    # for f in ['date', 'collection_date', 'submission_date']: # <= This is out of scope of responsibilities\n",
    "    #     if f in virus:\n",
    "    #         date_fields.append(f)\n",
    "\n",
    "    #for field in date_fields: # No...\n",
    "    \n",
    "    # If date_str is empty, return None\n",
    "    if date_str is None or date_str.strip() == '':\n",
    "        date_str = None\n",
    "        return\n",
    "        \n",
    "    date_str = re.sub(r'_', r'-', date_str)\n",
    "    # ex. 2002-XX-XX or 2002-09-05\n",
    "    if re.match(r'\\d\\d\\d\\d-(\\d\\d|XX)-(\\d\\d|XX)', date_str):\n",
    "        pass\n",
    "    # ex. 2002-2-4\n",
    "    elif re.match(r'^\\d\\d\\d\\d-\\d-\\d$', date_str):\n",
    "        date_str = re.sub(\n",
    "            r'^(\\d\\d\\d\\d)-(\\d)-(\\d)$', r'\\1-0\\2-0\\3', date_str)\n",
    "    # ex. 2002-02-4\n",
    "    elif re.match(r'^\\d\\d\\d\\d-\\d\\d-\\d$', date_str):\n",
    "        date_str = re.sub(\n",
    "            r'^(\\d\\d\\d\\d)-(\\d\\d)-(\\d)$', r'\\1-\\2-0\\3', date_str)\n",
    "    # ex. 2002-2-15\n",
    "    elif re.match(r'^\\d\\d\\d\\d-\\d-\\d\\d$', date_str):\n",
    "        date_str = re.sub(\n",
    "            r'^(\\d\\d\\d\\d)-(\\d)-(\\d\\d)$', r'\\1-0\\2-\\3', date_str)\n",
    "    elif re.match(r'\\d\\d\\d\\d\\s\\(Month\\sand\\sday\\sunknown\\)', date_str):\n",
    "        date_str = date_str[0:4] + \"-XX-XX\"\n",
    "    # ex. 2009-06 (Day unknown)\n",
    "    elif re.match(r'\\d\\d\\d\\d-\\d\\d\\s\\(Day\\sunknown\\)', date_str):\n",
    "        date_str = date_str[0:7] + \"-XX\"\n",
    "    elif re.match(r'\\d\\d\\d\\d-\\d\\d', date_str):\n",
    "        date_str = date_str[0:7] + \"-XX\"\n",
    "    elif re.match(r'\\d\\d\\d\\d', date_str):\n",
    "        date_str = date_str[0:4] + \"-XX-XX\"\n",
    "    else:\n",
    "        print(\"Couldn't reformat this date: \" +\n",
    "              date_str + \", setting to None\")\n",
    "        date_str = None\n",
    "    return date_str\n",
    "\n",
    "print(format_date(\"2002-XX-XX\"))\n",
    "print(format_date(\"2002-09-05\"))\n",
    "print(format_date(\"2002-09\"))\n",
    "print(format_date(\"2002-2-4\"))\n",
    "print(format_date(\"2002-02-4\"))\n",
    "print(format_date(\"2002-02-4\"))\n",
    "print(format_date(\"2002-02-4\"))\n",
    "print(format_date(\"2002-02-4\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff1aace7-0cab-4697-b99f-1bab901034bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-23\n",
      "2020-08-24\n",
      "2020-08-01\n",
      "2020-09\n",
      "2020-01-01\n",
      "2020-01-01\n",
      "2020-10-01\n",
      "2020-08-23\n",
      "2020-XX-1X (hi\n",
      "2020 (hi\n"
     ]
    }
   ],
   "source": [
    "def ncov_ingest_format_date(date_string: str, expected_formats: set) -> str:\n",
    "    \"\"\"\n",
    "    Format *date_string* to ISO 8601 date (YYYY-MM-DD).\n",
    "    If *date_string* does not match *expected_formats*, return *date_string*.\n",
    "    >>> expected_formats = {'%Y-%m-%d', '%Y-%m-%dT%H:%M:%SZ'}\n",
    "    >>> format_date(\"2020\", expected_formats)\n",
    "    '2020'\n",
    "    >>> format_date(\"2020-01\", expected_formats)\n",
    "    '2020-01'\n",
    "    >>> format_date(\"2020-1-15\", expected_formats)\n",
    "    '2020-01-15'\n",
    "    >>> format_date(\"2020-1-1\", expected_formats)\n",
    "    '2020-01-01'\n",
    "    >>> format_date(\"2020-01-15\", expected_formats)\n",
    "    '2020-01-15'\n",
    "    >>> format_date(\"2020-01-15T00:00:00Z\", expected_formats)\n",
    "    '2020-01-15'\n",
    "    >>> format_date(\"2020-XX-XX\", expected_formats)\n",
    "    '2020'\n",
    "    >>> format_date(\"2020 (Month and day unknown)\", expected_formats)\n",
    "    '2020'\n",
    "    >>> format_date(\"2020-06 (Day unknown)\", expected_formats)\n",
    "    '2020-06'\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    for date_format in expected_formats:\n",
    "        try:\n",
    "            date_string = date_string.replace(\" .*\",\"\")\n",
    "            return datetime.strptime(date_string, date_format).strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return date_string\n",
    "\n",
    "# ex. 2002-XX-XX or 2002-09-XX\n",
    "# ex. 2009-06 (Day unknown)\n",
    "# ex. 2009 (Month and day unknown)\n",
    "# https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior\n",
    "expected_date_formats = {'%Y_%m_%d','%Y (Month and day unknown)', '%Y-%m (Day unknown)', \n",
    "                         '%Y %b %d','%Y-XX-XX', '%Y-%m-%d', '%Y-%m-%dT%H:%M:%SZ'}\n",
    "\n",
    "print(ncov_ingest_format_date(\"2020_08_23\", expected_date_formats))\n",
    "print(ncov_ingest_format_date(\"2020-8-24\", expected_date_formats))\n",
    "print(ncov_ingest_format_date(\"2020-8-1\", expected_date_formats))\n",
    "print(ncov_ingest_format_date(\"2020-09\", expected_date_formats))\n",
    "print(ncov_ingest_format_date(\"2020-XX-XX\", expected_date_formats))  # resets to Jan 1st\n",
    "print(ncov_ingest_format_date(\"2020 (Month and day unknown)\", expected_date_formats))\n",
    "print(ncov_ingest_format_date(\"2020-10 (Day unknown)\", expected_date_formats)) # resets day to 1st\n",
    "print(ncov_ingest_format_date(\"2020 Aug 23\", expected_date_formats))\n",
    "print(ncov_ingest_format_date(\"2020-XX-1X (hi\", expected_date_formats))\n",
    "print(ncov_ingest_format_date(\"2020 (hi\", expected_date_formats))\n",
    "\n",
    "# Catch up on ambiguous dates thread\n",
    "# Hmm: https://github.com/nextstrain/augur/blob/master/augur/util_support/date_disambiguator.py\n",
    "# https://github.com/nextstrain/augur/issues/602\n",
    "# https://github.com/nextstrain/augur/pull/631"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde55b6-db6c-40db-92fc-c793deb6a4d4",
   "metadata": {},
   "source": [
    "Use Augur's date.py\n",
    "\n",
    "```\n",
    "git clone https://github.com/victorlin/augur.git\n",
    "cd augur\n",
    "git checkout filter/sqlite\n",
    "less augur/dates.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9a5c217-0e0d-416b-a089-126972bd7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "#import datetime\n",
    "import re\n",
    "import isodate\n",
    "import treetime\n",
    "from datetime import date\n",
    "from textwrap import dedent\n",
    "from functools import lru_cache\n",
    "from typing import Any, List\n",
    "\n",
    "\n",
    "SUPPORTED_DATE_HELP_TEXT = dedent(\"\"\"\\\n",
    "    1. an Augur-style numeric date with the year as the integer part (e.g. 2020.42) or\n",
    "    2. a date in ISO 8601 date format (i.e. YYYY-MM-DD) (e.g. '2020-06-04') or\n",
    "    3. a backwards-looking relative date in ISO 8601 duration format with optional P prefix (e.g. '1W', 'P1W')\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "class InvalidDateFormat(ValueError):\n",
    "    pass\n",
    "\n",
    "\n",
    "# float, negative ok\n",
    "# note: year-only is treated as incomplete ambiguous and must be non-negative (see RE_YEAR_ONLY)\n",
    "RE_NUMERIC_DATE = re.compile(r'^-*\\d+\\.\\d+$')\n",
    "\n",
    "# complete ISO 8601 date\n",
    "# e.g. 2018-03-25\n",
    "RE_ISO_8601_DATE = re.compile(r'^\\d{4}-\\d{2}-\\d{2}$')\n",
    "\n",
    "# complete ambiguous ISO 8601 date\n",
    "# e.g. 2018-03-XX\n",
    "RE_AMBIGUOUS_ISO_8601_DATE = re.compile(r'^[\\dX]{4}-[\\dX]{2}-[\\dX]{2}$')\n",
    "\n",
    "# incomplete ambiguous ISO 8601 date (missing day)\n",
    "# e.g. 2018-03\n",
    "RE_AMBIGUOUS_ISO_8601_DATE_YEAR_MONTH = re.compile(r'^[\\dX]{4}-[\\dX]{2}$')\n",
    "#RE_AMBIGUOUS_ISO_8601_DATE_YEAR_MONTH = re.compile(r'^[\\dX]{4}-[\\dX]{2}\\s')\n",
    "\n",
    "# incomplete ambiguous ISO 8601 date (missing month and day)\n",
    "# e.g. 2018\n",
    "# and other non-negative ints\n",
    "# e.g. 0, 1, 123, 12345\n",
    "RE_YEAR_ONLY = re.compile(r'^[\\dX]+$')\n",
    "\n",
    "# also support relative dates (ISO 8601 durations) - see any_to_numeric()\n",
    "\n",
    "\n",
    "CACHE_SIZE = 8192\n",
    "# Some functions below use @lru_cache to minimize redundant operations on\n",
    "# large datasets that are likely to have multiple entries with the same date value.\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=CACHE_SIZE)\n",
    "def get_year(date_in:Any):\n",
    "    \"\"\"Get the year from a date. Only works for ISO dates.\n",
    "\n",
    "    This function is intended to be registered as a user-defined function in sqlite3.\n",
    "    \"\"\"\n",
    "    date_in = str(date_in)\n",
    "    try:\n",
    "        return int(date_in.split('-')[0])\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=CACHE_SIZE)\n",
    "def get_month(date_in:Any):\n",
    "    \"\"\"Get the month from a date. Only works for ISO dates.\n",
    "\n",
    "    This function is intended to be registered as a user-defined function in sqlite3.\n",
    "    \"\"\"\n",
    "    date_in = str(date_in)\n",
    "    try:\n",
    "        return int(date_in.split('-')[1])\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=CACHE_SIZE)\n",
    "def get_day(date_in:Any):\n",
    "    \"\"\"Get the day from a date. Only works for ISO dates.\n",
    "\n",
    "    This function is intended to be registered as a user-defined function in sqlite3.\n",
    "    \"\"\"\n",
    "    date_in = str(date_in)\n",
    "    try:\n",
    "        return int(date_in.split('-')[2])\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def iso_to_numeric(date_in:str, ambiguity_resolver:str):\n",
    "    \"\"\"Convert ISO 8601 date string to numeric, resolving any ambiguity detected by explicit 'X' characters or missing date parts.\"\"\"\n",
    "    date_parts = date_in.split('-', maxsplit=2)\n",
    "    # TODO: resolve partial month/day ambiguity eg. 2018-1X-XX, 2018-10-3X\n",
    "    if ambiguity_resolver == 'min':\n",
    "        year = int(date_parts[0].replace('X', '0'))\n",
    "        month = int(date_parts[1]) if len(date_parts) > 1 and date_parts[1].isnumeric() else 1\n",
    "        day = int(date_parts[2]) if len(date_parts) > 2 and date_parts[2].isnumeric() else 1\n",
    "        return date_to_numeric_capped(date(year, month, day))\n",
    "    if ambiguity_resolver == 'max':\n",
    "        year = int(date_parts[0].replace('X', '9'))\n",
    "        month = int(date_parts[1]) if len(date_parts) > 1 and date_parts[1].isnumeric() else 12\n",
    "        if len(date_parts) == 3 and date_parts[2].isnumeric():\n",
    "            day = int(date_parts[2])\n",
    "        else:\n",
    "            if month in {1,3,5,7,8,10,12}:\n",
    "                day = 31\n",
    "            elif month == 2:\n",
    "                day = 28\n",
    "            else:\n",
    "                day = 30\n",
    "        return date_to_numeric_capped(date(year, month, day))\n",
    "\n",
    "\n",
    "def any_to_numeric(date_in:Any, ambiguity_resolver:str):\n",
    "    \"\"\"Return numeric date if date is in a supported format.\n",
    "\n",
    "    For ambiguous ISO 8601 dates, resolve to either minimum or maximum possible value.\n",
    "    \"\"\"\n",
    "    date_in = str(date_in)\n",
    "\n",
    "    # Absolute date in numeric format.\n",
    "    if RE_NUMERIC_DATE.match(date_in):\n",
    "        return float(date_in)\n",
    "\n",
    "    # Absolute date in potentially incomplete/ambiguous ISO 8601 date format.\n",
    "    if (RE_ISO_8601_DATE.match(date_in) or\n",
    "        RE_AMBIGUOUS_ISO_8601_DATE.match(date_in) or\n",
    "        RE_AMBIGUOUS_ISO_8601_DATE_YEAR_MONTH.match(date_in) or\n",
    "        RE_YEAR_ONLY.match(date_in)\n",
    "        ):\n",
    "        return iso_to_numeric(date_in, ambiguity_resolver)\n",
    "\n",
    "    # Relative date in ISO 8601 duration format.\n",
    "    # No regex for this (it is complex), just try evaluating last and\n",
    "    # let any expected errors pass to raise the general-purpose InvalidDateFormat.\n",
    "    try:\n",
    "        # make a copy of date_in for this block\n",
    "        duration_str = str(date_in)\n",
    "        if not duration_str.startswith('P'):\n",
    "            duration_str = 'P'+duration_str\n",
    "        return treetime.utils.numeric_date(datetime.date.today() - isodate.parse_duration(duration_str))\n",
    "    except (ValueError, isodate.ISO8601Error):\n",
    "        pass\n",
    "\n",
    "    raise InvalidDateFormat(f\"\"\"Unable to determine date from '{date_in}'. Ensure it is in one of the supported formats:\\n{SUPPORTED_DATE_HELP_TEXT}\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "def any_to_numeric_type_min(date_in:Any):\n",
    "    \"\"\"Get the numeric date from any supported date format, taking the minimum possible value if ambiguous.\n",
    "\n",
    "    This function is intended to be used as the `type` parameter in `argparse.ArgumentParser.add_argument()`\n",
    "\n",
    "    This raises an ArgumentTypeError from InvalidDateFormat exceptions, otherwise the custom exception message won't be shown in console output due to:\n",
    "    https://github.com/python/cpython/blob/5c4d1f6e0e192653560ae2941a6677fbf4fbd1f2/Lib/argparse.py#L2503-L2513\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return any_to_numeric(date_in, ambiguity_resolver='min')\n",
    "    except InvalidDateFormat as e:\n",
    "        raise argparse.ArgumentTypeError(str(e)) from e\n",
    "\n",
    "\n",
    "def any_to_numeric_type_max(date_in:Any):\n",
    "    \"\"\"Get the numeric date from any supported date format, taking the maximum possible value if ambiguous.\n",
    "\n",
    "    This function is intended to be used as the `type` parameter in `argparse.ArgumentParser.add_argument()`\n",
    "\n",
    "    This raises an ArgumentTypeError from InvalidDateFormat exceptions, otherwise the custom exception message won't be shown in console output due to:\n",
    "    https://github.com/python/cpython/blob/5c4d1f6e0e192653560ae2941a6677fbf4fbd1f2/Lib/argparse.py#L2503-L2513\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return any_to_numeric(date_in, ambiguity_resolver='max')\n",
    "    except InvalidDateFormat as e:\n",
    "        raise argparse.ArgumentTypeError(str(e)) from e\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=CACHE_SIZE)\n",
    "def try_get_numeric_date_min(date_in:Any):\n",
    "    \"\"\"Get the numeric date from any supported date format, taking the minimum possible value if ambiguous.\n",
    "\n",
    "    This function is intended to be registered as a user-defined function in sqlite3.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return any_to_numeric(date_in, ambiguity_resolver='min')\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=CACHE_SIZE)\n",
    "def try_get_numeric_date_max(date_in:Any):\n",
    "    \"\"\"Get the numeric date from any supported date format, taking the maximum possible value if ambiguous.\n",
    "\n",
    "    This function is intended to be registered as a user-defined function in sqlite3.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return any_to_numeric(date_in, ambiguity_resolver='max')\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=CACHE_SIZE)\n",
    "def get_date_errors(date_in:Any):\n",
    "    \"\"\"Check date for any errors.\n",
    "\n",
    "    assert_only_less_significant_ambiguity:\n",
    "\n",
    "    If an exception is raised here, it will result in a `sqlite3.OperationalError`\n",
    "    without trace to the original exception. For this reason, if the check raises\n",
    "    :class:`InvalidDateFormat`, return a constant string\n",
    "    `ASSERT_ONLY_LESS_SIGNIFICANT_AMBIGUITY_VALUE` which sqlite3 can then \"handle\".\n",
    "\n",
    "    This function is intended to be registered as a user-defined function in sqlite3.\n",
    "    \"\"\"\n",
    "    date_in = str(date_in)\n",
    "    if not date_in:\n",
    "        # let empty string pass silently\n",
    "        return None\n",
    "    if RE_NUMERIC_DATE.match(date_in):\n",
    "        # let numeric dates pass silently\n",
    "        return None\n",
    "    if date_in[0] == '-':\n",
    "        # let negative ISO dates pass silently\n",
    "        return None\n",
    "    date_parts = date_in.split('-', maxsplit=2)\n",
    "    try:\n",
    "        assert_only_less_significant_ambiguity(date_parts)\n",
    "    except InvalidDateFormat as e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "ASSERT_ONLY_LESS_SIGNIFICANT_AMBIGUITY_ERROR = 'assert_only_less_significant_ambiguity'\n",
    "\n",
    "\n",
    "def assert_only_less_significant_ambiguity(date_parts:List[str]):\n",
    "    \"\"\"\n",
    "    Raise an exception if a constrained digit appears in a less-significant place\n",
    "    than an uncertain digit.\n",
    "\n",
    "    These patterns are valid:\n",
    "        2000-01-01\n",
    "        2000-01-XX\n",
    "        2000-XX-XX\n",
    "\n",
    "    but this is invalid, because month is uncertain but day is constrained:\n",
    "        2000-XX-01\n",
    "\n",
    "    These invalid cases are assumed to be unintended use of the tool.\n",
    "    \"\"\"\n",
    "    has_exact_year = date_parts[0].isnumeric()\n",
    "    has_exact_month = len(date_parts) > 1 and date_parts[1].isnumeric()\n",
    "    has_exact_day = len(date_parts) > 2 and date_parts[2].isnumeric()\n",
    "    if has_exact_day and not (has_exact_month and has_exact_year):\n",
    "        raise InvalidDateFormat(ASSERT_ONLY_LESS_SIGNIFICANT_AMBIGUITY_ERROR)\n",
    "    if has_exact_month and not has_exact_year:\n",
    "        raise InvalidDateFormat(ASSERT_ONLY_LESS_SIGNIFICANT_AMBIGUITY_ERROR)\n",
    "\n",
    "\n",
    "\n",
    "### date_to_numeric logic ###\n",
    "# copied from treetime.utils.numeric_date\n",
    "# simplified+cached for speed\n",
    "\n",
    "from calendar import isleap\n",
    "def date_to_numeric(d:date):\n",
    "    \"\"\"Return the numeric date representation of a datetime.date.\"\"\"\n",
    "    days_in_year = 366 if isleap(d.year) else 365\n",
    "    return d.year + (d.timetuple().tm_yday-0.5) / days_in_year\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=CACHE_SIZE)\n",
    "def date_to_numeric_capped(d:date, max_numeric:float=date_to_numeric(date.today())):\n",
    "    \"\"\"Return the numeric date representation of a datetime.date, capped at a maximum numeric value.\"\"\"\n",
    "    d_numeric = date_to_numeric(d)\n",
    "    if d_numeric > max_numeric:\n",
    "        d_numeric = max_numeric\n",
    "    return d_numeric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b013c-6fbc-4014-9aa7-629060d5de64",
   "metadata": {},
   "source": [
    "## 3. Upload to fauna (nope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90bb86c-2746-4ce0-af5f-c320aaf706f1",
   "metadata": {},
   "source": [
    "## 3. BioPython reorg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c54c302-3b6d-40d3-8256-91db8419fb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KY241742|ZIKV_SG_072|NA|2016_08_28|Human|Singapore|Asian|Zika_virus\n",
      "['KY241742', 'ZIKV_SG_072', 'NA', '2016_08_28', 'Human', 'Singapore', 'Asian', 'Zika_virus']\n",
      "metadata= {'accession': 'KY241742', 'strain': 'ZIKV_SG_072', 'collection_date': '2016_08_28', 'host': 'Human', 'country': 'Singapore'}\n",
      "metadata[strain]= SG_072\n",
      "metadata[collection_date]= 2016-08-28\n",
      "MF098771|Mexico_Rus_12TVR_2017|NA|2017_01_30|Human|Russia|Asian|Zika_virus\n",
      "['MF098771', 'Mexico_Rus_12TVR_2017', 'NA', '2017_01_30', 'Human', 'Russia', 'Asian', 'Zika_virus']\n",
      "metadata= {'accession': 'MF098771', 'strain': 'Mexico_Rus_12TVR_2017', 'collection_date': '2017_01_30', 'host': 'Human', 'country': 'Russia'}\n",
      "metadata[strain]= Mexico_Rus_12TVR_2017\n",
      "metadata[collection_date]= 2017-01-30\n",
      "MF098768|Dominican_Rep_Rus_7EGR_2016|NA|2016_08_25|Human|Russia|Asian|Zika_virus\n",
      "['MF098768', 'Dominican_Rep_Rus_7EGR_2016', 'NA', '2016_08_25', 'Human', 'Russia', 'Asian', 'Zika_virus']\n",
      "metadata= {'accession': 'MF098768', 'strain': 'Dominican_Rep_Rus_7EGR_2016', 'collection_date': '2016_08_25', 'host': 'Human', 'country': 'Russia'}\n",
      "metadata[strain]= Dominican_Rep_Rus_7EGR_2016\n",
      "metadata[collection_date]= 2016-08-25\n",
      "MF098770|Mexico_Rus_10GNN_2016|NA|2016_11_09|Human|Russia|Asian|Zika_virus\n",
      "['MF098770', 'Mexico_Rus_10GNN_2016', 'NA', '2016_11_09', 'Human', 'Russia', 'Asian', 'Zika_virus']\n",
      "metadata= {'accession': 'MF098770', 'strain': 'Mexico_Rus_10GNN_2016', 'collection_date': '2016_11_09', 'host': 'Human', 'country': 'Russia'}\n",
      "metadata[strain]= Mexico_Rus_10GNN_2016\n",
      "metadata[collection_date]= 2016-11-09\n",
      "MF098767|Saint_Barthelemi_Rus_6BRN_2016|NA|2016_07_25|Human|Russia|Asian|Zika_virus\n",
      "['MF098767', 'Saint_Barthelemi_Rus_6BRN_2016', 'NA', '2016_07_25', 'Human', 'Russia', 'Asian', 'Zika_virus']\n",
      "metadata= {'accession': 'MF098767', 'strain': 'Saint_Barthelemi_Rus_6BRN_2016', 'collection_date': '2016_07_25', 'host': 'Human', 'country': 'Russia'}\n",
      "metadata[strain]= Saint_Barthelemi_Rus_6BRN_2016\n",
      "metadata[collection_date]= 2016-07-25\n",
      "MF098766|Dominican_Rep_Rus_5RMN_2016|NA|2016_05_31|Human|Russia|Asian|Zika_virus\n",
      "['MF098766', 'Dominican_Rep_Rus_5RMN_2016', 'NA', '2016_05_31', 'Human', 'Russia', 'Asian', 'Zika_virus']\n",
      "metadata= {'accession': 'MF098766', 'strain': 'Dominican_Rep_Rus_5RMN_2016', 'collection_date': '2016_05_31', 'host': 'Human', 'country': 'Russia'}\n",
      "metadata[strain]= Dominican_Rep_Rus_5RMN_2016\n",
      "metadata[collection_date]= 2016-05-31\n",
      "MF098769|Dominican_Rep_Rus_8ZBR_2016|NA|2016_08_25|Human|Russia|Asian|Zika_virus\n",
      "['MF098769', 'Dominican_Rep_Rus_8ZBR_2016', 'NA', '2016_08_25', 'Human', 'Russia', 'Asian', 'Zika_virus']\n",
      "metadata= {'accession': 'MF098769', 'strain': 'Dominican_Rep_Rus_8ZBR_2016', 'collection_date': '2016_08_25', 'host': 'Human', 'country': 'Russia'}\n",
      "metadata[strain]= Dominican_Rep_Rus_8ZBR_2016\n",
      "metadata[collection_date]= 2016-08-25\n",
      "KY967711|SY01_2016|NA|2016_11_01|Human|China|Asian|Zika_virus\n",
      "['KY967711', 'SY01_2016', 'NA', '2016_11_01', 'Human', 'China', 'Asian', 'Zika_virus']\n",
      "metadata= {'accession': 'KY967711', 'strain': 'SY01_2016', 'collection_date': '2016_11_01', 'host': 'Human', 'country': 'China'}\n",
      "metadata[strain]= SY01_2016\n",
      "metadata[collection_date]= 2016-11-01\n",
      "KX051561|SK403/13AS|NA|2013_09_21|Human|Thailand|Asian|Zika_virus\n",
      "['KX051561', 'SK403/13AS', 'NA', '2013_09_21', 'Human', 'Thailand', 'Asian', 'Zika_virus']\n",
      "metadata= {'accession': 'KX051561', 'strain': 'SK403/13AS', 'collection_date': '2013_09_21', 'host': 'Human', 'country': 'Thailand'}\n",
      "metadata[strain]= SK403/13AS\n",
      "metadata[collection_date]= 2013-09-21\n",
      "KX051560|SK364/13AS|NA|2013_07_09|Human|Thailand|Asian|Zika_virus\n",
      "['KX051560', 'SK364/13AS', 'NA', '2013_07_09', 'Human', 'Thailand', 'Asian', 'Zika_virus']\n",
      "metadata= {'accession': 'KX051560', 'strain': 'SK364/13AS', 'collection_date': '2013_07_09', 'host': 'Human', 'country': 'Thailand'}\n",
      "metadata[strain]= SK364/13AS\n",
      "metadata[collection_date]= 2013-07-09\n",
      "Variable                                       Type                    Data/Info\n",
      "--------------------------------------------------------------------------------\n",
      "ASSERT_ONLY_LESS_SIGNIFICANT_AMBIGUITY_ERROR   str                     assert_only_less_significant_ambiguity\n",
      "Any                                            _SpecialForm            typing.Any\n",
      "CACHE_SIZE                                     int                     8192\n",
      "InvalidDateFormat                              type                    <class '__main__.InvalidDateFormat'>\n",
      "List                                           _SpecialGenericAlias    typing.List\n",
      "NamedTuple                                     function                <function NamedTuple at 0x107af68c0>\n",
      "RE_AMBIGUOUS_ISO_8601_DATE                     Pattern                 re.compile('^[\\\\dX]{4}-[\\\\dX]{2}-[\\\\dX]{2}$')\n",
      "RE_AMBIGUOUS_ISO_8601_DATE_YEAR_MONTH          Pattern                 re.compile('^[\\\\dX]{4}-[\\\\dX]{2}$')\n",
      "RE_ISO_8601_DATE                               Pattern                 re.compile('^\\\\d{4}-\\\\d{2}-\\\\d{2}$')\n",
      "RE_NUMERIC_DATE                                Pattern                 re.compile('^-*\\\\d+\\\\.\\\\d+$')\n",
      "RE_YEAR_ONLY                                   Pattern                 re.compile('^[\\\\dX]+$')\n",
      "SUPPORTED_DATE_HELP_TEXT                       str                     1. an Augur-style numeric<...>efix (e.g. '1W', 'P1W')\\n\n",
      "SeqIO                                          module                  <module 'Bio.SeqIO' from <...>s/Bio/SeqIO/__init__.py'>\n",
      "any_to_numeric                                 function                <function any_to_numeric at 0x1102d2200>\n",
      "any_to_numeric_type_max                        function                <function any_to_numeric_type_max at 0x1102d2320>\n",
      "any_to_numeric_type_min                        function                <function any_to_numeric_type_min at 0x1102d2290>\n",
      "argparse                                       module                  <module 'argparse' from '<...>/python3.10/argparse.py'>\n",
      "assert_only_less_significant_ambiguity         function                <function assert_only_les<...>ambiguity at 0x1102d23b0>\n",
      "content                                        list                    n=8\n",
      "csv                                            module                  <module 'csv' from '/User<...>b/lib/python3.10/csv.py'>\n",
      "date                                           type                    <class 'datetime.date'>\n",
      "date_fix_fname                                 str                     zika_date_fix.tsv\n",
      "date_to_numeric                                function                <function date_to_numeric at 0x1102d27a0>\n",
      "date_to_numeric_capped                         _lru_cache_wrapper      <functools._lru_cache_wra<...>er object at 0x1102f87d0>\n",
      "datetime                                       type                    <class 'datetime.datetime'>\n",
      "dedent                                         function                <function dedent at 0x108a1e440>\n",
      "define_fixes_dict                              function                <function define_fixes_dict at 0x10b4d28c0>\n",
      "expected_date_formats                          set                     {'%Y-%m (Day unknown)', '<...>, '%Y %b %d', '%Y_%m_%d'}\n",
      "fhandle                                        TextIOWrapper           <_io.TextIOWrapper name='<...>ode='r' encoding='UTF-8'>\n",
      "fix_casing                                     function                <function fix_casing at 0x10b4d2e60>\n",
      "fix_date_dict                                  dict                    n=1\n",
      "fix_location_dict                              dict                    n=51\n",
      "fix_name_dict                                  dict                    n=106\n",
      "fixes_str                                      function                <function fixes_str at 0x10b4d2950>\n",
      "fixes_strain_name                              function                <function fixes_strain_name at 0x10b4d2dd0>\n",
      "fname                                          str                     ../example_data/small.fasta\n",
      "format_date                                    function                <function format_date at 0x10a49fbe0>\n",
      "get_date_errors                                _lru_cache_wrapper      <functools._lru_cache_wra<...>er object at 0x1102f8300>\n",
      "get_day                                        _lru_cache_wrapper      <functools._lru_cache_wra<...>er object at 0x1102d7950>\n",
      "get_month                                      _lru_cache_wrapper      <functools._lru_cache_wra<...>er object at 0x1102d7740>\n",
      "get_year                                       _lru_cache_wrapper      <functools._lru_cache_wra<...>er object at 0x1102d7530>\n",
      "header_fasta_fields                            dict                    n=5\n",
      "isleap                                         function                <function isleap at 0x108b21090>\n",
      "iso_to_numeric                                 function                <function iso_to_numeric at 0x10b4d2f80>\n",
      "isodate                                        module                  <module 'isodate' from '/<...>ges/isodate/__init__.py'>\n",
      "location_fix_fname                             str                     zika_location_fix.tsv\n",
      "lru_cache                                      function                <function lru_cache at 0x107a11d80>\n",
      "metadata                                       dict                    n=5\n",
      "mhandle                                        TextIOWrapper           <_io.TextIOWrapper name='<...>ode='a' encoding='UTF-8'>\n",
      "ncov_ingest_format_date                        function                <function ncov_ingest_format_date at 0x10b4d2710>\n",
      "os                                             module                  <module 'os' from '/Users<...>ab/lib/python3.10/os.py'>\n",
      "re                                             module                  <module 're' from '/Users<...>ab/lib/python3.10/re.py'>\n",
      "record                                         SeqRecord               ID: KX051560|SK364/13AS|N<...>AGCGAAAGCTAGCAACA...ATG')\n",
      "shandle                                        TextIOWrapper           <_io.TextIOWrapper name='<...>ode='a' encoding='UTF-8'>\n",
      "strain_fix_fname                               str                     zika_strain_name_fix.tsv\n",
      "sys                                            module                  <module 'sys' (built-in)>\n",
      "time                                           module                  <module 'time' (built-in)>\n",
      "treetime                                       module                  <module 'treetime' from '<...>es/treetime/__init__.py'>\n",
      "try_get_numeric_date_max                       _lru_cache_wrapper      <functools._lru_cache_wra<...>er object at 0x1102f80f0>\n",
      "try_get_numeric_date_min                       _lru_cache_wrapper      <functools._lru_cache_wra<...>er object at 0x1102d7e20>\n",
      "virus                                          str                     zika\n",
      "zika_fasta                                     str                     ../example_data/small.fasta\n"
     ]
    }
   ],
   "source": [
    "fname=zika_fasta\n",
    "\n",
    "# Early exit if file not found\n",
    "try:\n",
    "    fhandle = open(fname, 'r')\n",
    "except IOError:\n",
    "    raise Exception(fname, \"not found\")\n",
    "\n",
    "fix_name_dict = define_fixes_dict(strain_fix_fname) \n",
    "fix_location_dict = define_fixes_dict(location_fix_fname)\n",
    "fix_date_dict = define_fixes_dict(date_fix_fname)\n",
    "    \n",
    "try:\n",
    "    shandle = open(\"sequences.fasta\", 'w')\n",
    "    mhandle = open(\"metadata.tsv\", 'w')\n",
    "    shandle.close()\n",
    "    mhandle.close()\n",
    "    shandle = open(\"sequences.fasta\", 'a')\n",
    "    mhandle = open(\"metadata.tsv\", 'a')\n",
    "    mhandle.write(\"\\t\".join((\"strain\", \"virus\", \"accession\", \"date\", \"region\", \"country\", \"division\", \"city\", \"db\", \"segment\", \"authors\", \"url\", \"title\", \"journal\", \"paper_url\"))+\"\\n\")\n",
    "except IOError:\n",
    "    raise Exception('Cannot write to sequences.fasta and/or metadata.tsv')\n",
    "\n",
    "for record in SeqIO.parse(fhandle, \"fasta\"):\n",
    "    #print(record.id) # Header, Breaks at spaces!\n",
    "    print(record.description) # Whole header\n",
    "    content = list(\n",
    "        map(lambda x: x.strip(), \n",
    "            record.description\n",
    "            .replace(\" \", \"_\") # Deal with spaces\n",
    "            .split('|'))\n",
    "    )\n",
    "    print(content)\n",
    "    metadata = {key: content[ii] if ii < len(content) else \"\" for ii, key in header_fasta_fields.items()}\n",
    "    print(\"metadata=\", metadata)\n",
    "    metadata[\"strain\"] = fixes_strain_name(metadata[\"strain\"], fixes_dict=fix_name_dict)\n",
    "    # metadata[\"collection_date\"] = fixes_str(metadata[\"strain\"], fix_date_dict) # based on fixed strain name?\n",
    "    # metadata[\"location\"] = fixes_str(metadata[\"strain\"], fix_location_dict)  # find where location is defined\n",
    "    print(\"metadata[strain]=\", metadata[\"strain\"])\n",
    "    \n",
    "    # Hmm, was an obj method (checking for \"date\", \"collection date\", \"submission date\", seems too specialized...)\n",
    "    # If you want to check for all dates, then check all fields for a XXXX-XX-XX or similar date format...\n",
    "    \n",
    "    metadata[\"collection_date\"]=ncov_ingest_format_date(metadata[\"collection_date\"], expected_date_formats)\n",
    "    print(\"metadata[collection_date]=\", metadata[\"collection_date\"])\n",
    "    \n",
    "    # Stream a fasta file, do not read all into memory\n",
    "    # Append to metadata file\n",
    "    # Append to sequence file\n",
    "    shandle.write(\">\" + metadata[\"strain\"] + \"\\n\")\n",
    "    shandle.write(str(record.seq) + \"\\n\") # split in lines of 80 char\n",
    "    mhandle.write(\"\\t\".join((metadata[\"strain\"], \"zika\", metadata[\"accession\"], metadata[\"collection_date\"]))+\"\\n\")\n",
    "\n",
    "shandle.close()\n",
    "mhandle.close()\n",
    "fhandle.close()\n",
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3174ad08-3c81-4a48-8f07-2cbcd45438ed",
   "metadata": {},
   "source": [
    "# Aiming at a sequence and metadata file\n",
    "\n",
    "**sequence.fasta**\n",
    "\n",
    "Only strain name in header\n",
    "\n",
    "```\n",
    ">PAN/CDC_259359_V1_V3/2015\n",
    "gaatttgaagcgaatgctaacaacagtatcaacaggttttattttggatttggaaacgag\n",
    "agtttctggtcatgaaaaacccaaaaaagaaatccggaggattccggattgtcaatatgc\n",
    "...\n",
    "```\n",
    "\n",
    "**metadata.tsv**\n",
    "\n",
    "```\n",
    "strain  virus   accession       date    region  country division        city    db      segment authors url     title   journal paper_url\n",
    "PAN/CDC_259359_V1_V3/2015       zika    KX156774        2015-12-18      North America   Panama  Panama  Panama  genbank genome  Shabman et al   https://www.ncbi.nlm.nih.gov/nuccore/KX156774       Direct Submission       Submitted (29-APR-2016) J. Craig Venter Institute, 9704 Medical Center Drive, Rockville, MD 20850, USA  https://www.ncbi.nlm.nih.gov/pubmed/\n",
    "...\n",
    "```\n",
    "\n",
    "which can also look like:\n",
    "\n",
    "```\n",
    "strain=PAN/CDC_259359_V1_V3/2015=\n",
    "virus=zika\n",
    "accession=KX156774\n",
    "date=2015-12-18\n",
    "region=North America\n",
    "country=Panama\n",
    "division=Panama\n",
    "city=Panama\n",
    "db=genbank\n",
    "segment=genome\n",
    "authors=Shabman et al\n",
    "url=https://www.ncbi.nlm.nih.gov/nuccore/KX156774\n",
    "title=Direct Submission\n",
    "journal=Submitted (29-APR-2016) J. Craig Venter Institute, 9704 Medical Center Drive, Rockville, MD 20850, USA\n",
    "paper_url=https://www.ncbi.nlm.nih.gov/pubmed/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3575b-9710-4785-8aea-e3c72d471987",
   "metadata": {},
   "source": [
    "### Zika Update (genbank sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17ab11d7-efe1-403f-a0a2-43bf3d86fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_citations(self, database, table, preview, index='accession', **kwargs):\n",
    "#   print(\"Updating citation fields\")\n",
    "#   _, sequences = self.get_genbank_sequences(**kwargs)\n",
    "#   citation_keys = ['authors', 'title', 'journal', 'puburl', 'url', index]\n",
    "#   sequences = [{key: doc[key] for key in citation_keys} for doc in sequences]\n",
    "#   if not preview:\n",
    "#     print(\"Updating \" + str(len(sequences)) + \" sequence citations in \" + database + \".\" + table)\n",
    "#     self.upload_to_rethinkdb(database, table, sequences, overwrite=True, index='accession')\n",
    "#   else:\n",
    "#     print(\"Preview of updates to be made, remove --preview to make updates to database\")\n",
    "\n",
    "# def get_genbank_sequences(self, email, **kwargs):\n",
    "#     if self.accessions is None:\n",
    "#         table = self.virus + \"_sequences\"\n",
    "#         accessions = self.get_accessions(self.database, table)\n",
    "#     else:\n",
    "#         accessions = [acc.strip() for acc in self.accessions.split(\",\")]\n",
    "#     self.entrez_email(email)\n",
    "#     gi = self.get_GIs(accessions, kwargs[\"n_entrez\"])\n",
    "#     return self.get_entrez_viruses(gi, **kwargs)\n",
    "\n",
    "# def get_accessions(self, database, table):\n",
    "#     '''\n",
    "#     Return documents from the table.database for which accession numbers are from genbank\n",
    "#     '''\n",
    "#     print(\"Getting accession numbers for sequences obtained from Genbank\")\n",
    "#     accessions = list(r.db(database).table(table).filter((r.row[\"source\"] == 'genbank') | (r.row[\"source\"] == 'vipr')).get_field('accession').run())\n",
    "#     return accessions\n",
    "\n",
    "# def update_locations(self, database, table, preview, **kwargs):\n",
    "#     print(\"Updating location fields\")\n",
    "#     viruses = list(r.table(table).run())\n",
    "#     self.define_countries(\"source-data/geo_synonyms.tsv\")\n",
    "#     self.define_regions(\"source-data/geo_regions.tsv\")\n",
    "#     viruses = self.reassign_new_locations(viruses, self.location_fields, **kwargs)\n",
    "#     if not preview:\n",
    "#         print(\"Updating \" + str(len(viruses)) + \" virus locations in \" + self.database + \".\" + self.viruses_table)\n",
    "#         del kwargs['overwrite']\n",
    "#         self.upload_to_rethinkdb(database, table, viruses, overwrite=True, index='strain')\n",
    "#     else:\n",
    "#         print(\"Preview of updates to be made, remove --preview to make updates to database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7133a7a4-a6ac-487e-ab81-40da360cc7b1",
   "metadata": {},
   "source": [
    "# Genbank Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5382b1f-21d7-4bf9-b50b-fbf85813b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DbList': ['pubmed', 'protein', 'nuccore', 'ipg', 'nucleotide', 'structure', 'genome', 'annotinfo', 'assembly', 'bioproject', 'biosample', 'blastdbinfo', 'books', 'cdd', 'clinvar', 'gap', 'gapplus', 'grasp', 'dbvar', 'gene', 'gds', 'geoprofiles', 'homologene', 'medgen', 'mesh', 'ncbisearch', 'nlmcatalog', 'omim', 'orgtrack', 'pmc', 'popset', 'proteinclusters', 'pcassay', 'protfam', 'pccompound', 'pcsubstance', 'seqannot', 'snp', 'sra', 'taxonomy', 'biocollections', 'gtr']}\n"
     ]
    }
   ],
   "source": [
    "from Bio import Entrez\n",
    "Entrez.email = \"Your.Name.Here@example.org\"\n",
    "handle = Entrez.einfo() # or esearch, efetch, ...\n",
    "record = Entrez.read(handle)\n",
    "print(record)\n",
    "handle.close()\n",
    "\n",
    "# Hmm fetch in batches, which won't work if we're outputing the metadata one record at a time, unless we batch the metadata output.\n",
    "\n",
    "\n",
    "#     def get_entrez_viruses(self, giList, **kwargs):\n",
    "#         '''\n",
    "#         Use entrez efetch to get genbank entries from genbank identifiers\n",
    "#         '''\n",
    "#         ## define batch size for download\n",
    "#         batchSize = 100\n",
    "\n",
    "#         # post NCBI query\n",
    "#         try:\n",
    "#             search_handle = Entrez.epost(db=self.gbdb, id=\",\".join(giList))\n",
    "#             search_results = Entrez.read(search_handle)\n",
    "#             webenv, query_key = search_results[\"WebEnv\"], search_results[\"QueryKey\"]\n",
    "#         except:\n",
    "#             print(\"ERROR: Couldn't connect with entrez, please run again\")\n",
    "\n",
    "#         viruses = []\n",
    "#         sequences = []\n",
    "#         #fetch all results in batch of batchSize entries at once\n",
    "#         for start in range(0,len(giList),batchSize):\n",
    "#             #fetch entries in batch\n",
    "#             try:\n",
    "#                 handle = Entrez.efetch(db=self.gbdb, rettype=\"gb\", retstart=start, retmax=batchSize, webenv=webenv, query_key=query_key)\n",
    "#             except IOError:\n",
    "#                 print(\"ERROR: Couldn't connect with entrez, please run again\")\n",
    "#             else:\n",
    "#                 result = self.parse_gb_entries(handle, **kwargs)\n",
    "#                 viruses.extend(result[0])\n",
    "#                 sequences.extend(result[1])\n",
    "#         return (viruses, sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7756d9-c0ea-45e1-9f3f-1823510df07a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyterlab] *",
   "language": "python",
   "name": "conda-env-jupyterlab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
